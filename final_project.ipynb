{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb4b1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = 'energydata'\n",
    "\n",
    "all_files = [f for f in os.listdir(folder_path) if f.endswith('_hourly.csv') and 'pjm' not in f.lower()]\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for file in all_files:\n",
    "    region_name = file.split('_')[0]\n",
    "    df = pd.read_csv(os.path.join(folder_path, file))\n",
    "    df['Region'] = region_name\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.rename(columns={df.columns[1]: 'Demand'}, inplace=True)\n",
    "    data_list.append(df)\n",
    "\n",
    "combined_data = pd.concat(data_list, ignore_index=True)\n",
    "combined_data.sort_values(by='Datetime', inplace=True)\n",
    "combined_data.to_csv('combined_regions_hourly.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7c2db4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#randomly generated supply data but not in use \n",
    "np.random.seed(42)\n",
    "\n",
    "combined_data['WindSpeed'] = np.random.normal(5, 2, len(combined_data))\n",
    "combined_data['SolarRadiation'] = combined_data['Datetime'].dt.hour.apply(lambda x: 300 if 6 <= x <= 18 else 0)\n",
    "\n",
    "combined_data['SolarProduction'] = combined_data['SolarRadiation'] * 0.5\n",
    "combined_data['WindProduction'] = np.minimum(combined_data['WindSpeed'] * 2, 10)\n",
    "combined_data['FossilProduction'] = np.maximum(combined_data['Demand'] - \n",
    "                                               (combined_data['SolarProduction'] + combined_data['WindProduction']), 0)\n",
    "\n",
    "\n",
    "combined_data.to_csv('final_regions_with_simulation.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2573808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# base on the demxand generated the supply data and design 30% change over supply\n",
    "overproduction_probability = 0.3\n",
    "adjustments = np.random.choice([-1, 1], size=len(data), p=[1 - overproduction_probability, overproduction_probability])\n",
    "\n",
    "# simulate the supply data\n",
    "data['Supply'] = data['Demand'] + adjustments * np.random.uniform(0, 1000, len(data))\n",
    "\n",
    "# add extra noise makes the simulation more natural\n",
    "data['Supply'] += np.random.normal(0, 100, len(data))\n",
    "\n",
    "#initialize price as 100\n",
    "data['Price'] = 100  \n",
    "\n",
    "# calculate the imbalance\n",
    "data['Imbalance'] = data['Supply'] - data['Demand']\n",
    "\n",
    "# define state\n",
    "imbalance_bins = np.linspace(data['Imbalance'].min(), data['Imbalance'].max(), 10)\n",
    "data['State'] = np.digitize(data['Imbalance'], imbalance_bins) - 1 \n",
    "\n",
    "data[['Datetime', 'Demand', 'Supply', 'Imbalance', 'Price', 'State']].to_csv('simulated_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8bff6ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 264170\n",
      "Episode 2: Total Reward = 258610\n",
      "Episode 3: Total Reward = 262210\n",
      "Episode 4: Total Reward = 257150\n",
      "Episode 5: Total Reward = 258870\n",
      "Episode 6: Total Reward = 258730\n",
      "Episode 7: Total Reward = 256670\n",
      "Episode 8: Total Reward = 257630\n",
      "Episode 9: Total Reward = 257270\n",
      "Episode 10: Total Reward = 261350\n",
      "Episode 11: Total Reward = 257510\n",
      "Episode 12: Total Reward = 256530\n",
      "Episode 13: Total Reward = 255110\n",
      "Episode 14: Total Reward = 254270\n",
      "Episode 15: Total Reward = 259110\n",
      "Episode 16: Total Reward = 258230\n",
      "Episode 17: Total Reward = 255930\n",
      "Episode 18: Total Reward = 258250\n",
      "Episode 19: Total Reward = 254930\n",
      "Episode 20: Total Reward = 256830\n",
      "Episode 21: Total Reward = 254070\n",
      "Episode 22: Total Reward = 255290\n",
      "Episode 23: Total Reward = 256790\n",
      "Episode 24: Total Reward = 258330\n",
      "Episode 25: Total Reward = 253830\n",
      "Episode 26: Total Reward = 255250\n",
      "Episode 27: Total Reward = 257250\n",
      "Episode 28: Total Reward = 255650\n",
      "Episode 29: Total Reward = 258810\n",
      "Episode 30: Total Reward = 257950\n",
      "Episode 31: Total Reward = 257210\n",
      "Episode 32: Total Reward = 255750\n",
      "Episode 33: Total Reward = 256470\n",
      "Episode 34: Total Reward = 255650\n",
      "Episode 35: Total Reward = 254250\n",
      "Episode 36: Total Reward = 257850\n",
      "Episode 37: Total Reward = 255650\n",
      "Episode 38: Total Reward = 254750\n",
      "Episode 39: Total Reward = 253550\n",
      "Episode 40: Total Reward = 253990\n",
      "Episode 41: Total Reward = 253690\n",
      "Episode 42: Total Reward = 255810\n",
      "Episode 43: Total Reward = 254950\n",
      "Episode 44: Total Reward = 255850\n",
      "Episode 45: Total Reward = 254750\n",
      "Episode 46: Total Reward = 254570\n",
      "Episode 47: Total Reward = 257370\n",
      "Episode 48: Total Reward = 254930\n",
      "Episode 49: Total Reward = 255490\n",
      "Episode 50: Total Reward = 254150\n",
      "Episode 51: Total Reward = 252770\n",
      "Episode 52: Total Reward = 254470\n",
      "Episode 53: Total Reward = 256690\n",
      "Episode 54: Total Reward = 255730\n",
      "Episode 55: Total Reward = 256930\n",
      "Episode 56: Total Reward = 252990\n",
      "Episode 57: Total Reward = 253570\n",
      "Episode 58: Total Reward = 250810\n",
      "Episode 59: Total Reward = 254070\n",
      "Episode 60: Total Reward = 252730\n",
      "Episode 61: Total Reward = 255510\n",
      "Episode 62: Total Reward = 253390\n",
      "Episode 63: Total Reward = 255970\n",
      "Episode 64: Total Reward = 254730\n",
      "Episode 65: Total Reward = 255370\n",
      "Episode 66: Total Reward = 256270\n",
      "Episode 67: Total Reward = 256210\n",
      "Episode 68: Total Reward = 253950\n",
      "Episode 69: Total Reward = 255850\n",
      "Episode 70: Total Reward = 256730\n",
      "Episode 71: Total Reward = 256030\n",
      "Episode 72: Total Reward = 252350\n",
      "Episode 73: Total Reward = 254370\n",
      "Episode 74: Total Reward = 255490\n",
      "Episode 75: Total Reward = 254930\n",
      "Episode 76: Total Reward = 254570\n",
      "Episode 77: Total Reward = 254910\n",
      "Episode 78: Total Reward = 255510\n",
      "Episode 79: Total Reward = 252210\n",
      "Episode 80: Total Reward = 255530\n",
      "Episode 81: Total Reward = 255430\n",
      "Episode 82: Total Reward = 254850\n",
      "Episode 83: Total Reward = 256650\n",
      "Episode 84: Total Reward = 254850\n",
      "Episode 85: Total Reward = 256550\n",
      "Episode 86: Total Reward = 255670\n",
      "Episode 87: Total Reward = 253290\n",
      "Episode 88: Total Reward = 253470\n",
      "Episode 89: Total Reward = 256310\n",
      "Episode 90: Total Reward = 253810\n",
      "Episode 91: Total Reward = 255810\n",
      "Episode 92: Total Reward = 257490\n",
      "Episode 93: Total Reward = 256030\n",
      "Episode 94: Total Reward = 253450\n",
      "Episode 95: Total Reward = 256150\n",
      "Episode 96: Total Reward = 254390\n",
      "Episode 97: Total Reward = 256090\n",
      "Episode 98: Total Reward = 256190\n",
      "Episode 99: Total Reward = 254390\n",
      "Episode 100: Total Reward = 254670\n",
      "price: 0        84.232517\n",
      "1       129.680461\n",
      "2       114.797153\n",
      "3       150.000000\n",
      "4       109.350000\n",
      "           ...    \n",
      "1995     93.591686\n",
      "1996     80.000000\n",
      "1997     80.000000\n",
      "1998    132.313500\n",
      "1999    100.000000\n",
      "Name: Price, Length: 2000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = data.iloc[:2000].reset_index(drop=True)\n",
    "def adjust_price_and_demand(row, action):\n",
    "    min_price, max_price = 80, 150\n",
    "    \n",
    "    if action == 0:  # Increase Price\n",
    "        new_price = min(row['Price'] * 1.1, max_price) \n",
    "        new_demand = max(row['Demand'] * 0.9, 0)  \n",
    "    elif action == 1:  # Decrease Price\n",
    "        new_price = max(row['Price'] * 0.9, min_price)  \n",
    "        new_demand = row['Demand'] * 1.1 \n",
    "    else:  # Hold Price\n",
    "        new_price = row['Price']\n",
    "        new_demand = row['Demand']\n",
    "    \n",
    "    return new_price, new_demand\n",
    "\n",
    "\n",
    "\n",
    "def calculate_reward_with_price(imbalance, fossil_production, price, action):\n",
    "    reward = 0\n",
    "\n",
    "    if abs(imbalance) < 200:  \n",
    "        reward += 100\n",
    "    if 80 <= price <= 150:\n",
    "        reward += 100\n",
    "    else:\n",
    "        reward -= 50\n",
    "    \n",
    "    if action == 0:  # Increase Price\n",
    "        reward += 50 if imbalance > 50 else -10\n",
    "    elif action == 1:  # Decrease Price\n",
    "        reward += 50 if imbalance < -50 else -10\n",
    "    elif action == 2:  # Hold Price\n",
    "        reward += 50 if abs(imbalance) < 50 else -10\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "#Q-Learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_space, action_space, learning_rate=0.1, discount_factor=0.8, exploration_rate=1.0):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate  \n",
    "        self.q_table = np.zeros((state_space, action_space))  \n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return np.random.randint(self.action_space)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  \n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state, done):\n",
    "        best_next_action = np.argmax(self.q_table[next_state]) \n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action] * (1 - done)\n",
    "        td_error = td_target - self.q_table[state][action]  \n",
    "        self.q_table[state][action] += self.learning_rate * td_error \n",
    "\n",
    "state_space = len(imbalance_bins)  \n",
    "action_space = 3  \n",
    "agent = QLearningAgent(\n",
    "    state_space=state_space,\n",
    "    action_space=action_space,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.8, \n",
    "    exploration_rate=1.0  \n",
    ")\n",
    "\n",
    "num_episodes = 100\n",
    "episode_rewards = []  \n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    total_reward = 0\n",
    "\n",
    "    for idx in range(len(data) - 1):\n",
    "        current_row = data.iloc[idx]\n",
    "        next_row = data.iloc[idx + 1]\n",
    "        state = current_row['State']\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        new_price, new_demand = adjust_price_and_demand(current_row, action)\n",
    "        data.at[idx, 'Price'] = new_price  # update price each step\n",
    "        data.at[idx, 'Demand'] = new_demand  # update demand each step\n",
    "\n",
    "        adjusted_supply = current_row['Supply']\n",
    "        imbalance = adjusted_supply - new_demand\n",
    "\n",
    "        reward = calculate_reward_with_price(imbalance, current_row['Supply'], new_price, action)\n",
    "\n",
    "        # get next state\n",
    "        next_state = next_row['State']\n",
    "        done = idx == len(data) - 2\n",
    "\n",
    "        # update Q table\n",
    "        agent.update_q_value(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    \n",
    "\n",
    "\n",
    "print(\"price:\",data['Price'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd243c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be18d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
