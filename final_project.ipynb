{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb4b1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = 'energydata'\n",
    "\n",
    "all_files = [f for f in os.listdir(folder_path) if f.endswith('_hourly.csv') and 'pjm' not in f.lower()]\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for file in all_files:\n",
    "    region_name = file.split('_')[0]\n",
    "    df = pd.read_csv(os.path.join(folder_path, file))\n",
    "    df['Region'] = region_name\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.rename(columns={df.columns[1]: 'Demand'}, inplace=True)\n",
    "    data_list.append(df)\n",
    "\n",
    "combined_data = pd.concat(data_list, ignore_index=True)\n",
    "combined_data.sort_values(by='Datetime', inplace=True)\n",
    "combined_data.to_csv('combined_regions_hourly.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7c2db4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#randomly generated supply data but not in use \n",
    "np.random.seed(42)\n",
    "\n",
    "combined_data['WindSpeed'] = np.random.normal(5, 2, len(combined_data))\n",
    "combined_data['SolarRadiation'] = combined_data['Datetime'].dt.hour.apply(lambda x: 300 if 6 <= x <= 18 else 0)\n",
    "\n",
    "combined_data['SolarProduction'] = combined_data['SolarRadiation'] * 0.5\n",
    "combined_data['WindProduction'] = np.minimum(combined_data['WindSpeed'] * 2, 10)\n",
    "combined_data['FossilProduction'] = np.maximum(combined_data['Demand'] - \n",
    "                                               (combined_data['SolarProduction'] + combined_data['WindProduction']), 0)\n",
    "\n",
    "\n",
    "combined_data.to_csv('final_regions_with_simulation.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "15d96a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# base on the demxand generated the supply data and design 30% change over supply\n",
    "overproduction_probability = 0.3\n",
    "adjustments = np.random.choice([-1, 1], size=len(data), p=[1 - overproduction_probability, overproduction_probability])\n",
    "\n",
    "# simulate the supply data\n",
    "data['Supply'] = data['Demand'] + adjustments * np.random.uniform(0, 1000, len(data))\n",
    "\n",
    "# add extra noise makes the simulation more natural\n",
    "data['Supply'] += np.random.normal(0, 100, len(data))\n",
    "\n",
    "#initialize price as 100\n",
    "data['Price'] = 100  \n",
    "\n",
    "# calculate the imbalance\n",
    "data['Imbalance'] = data['Supply'] - data['Demand']\n",
    "\n",
    "# define state\n",
    "imbalance_bins = np.linspace(data['Imbalance'].min(), data['Imbalance'].max(), 10)\n",
    "data['State'] = np.digitize(data['Imbalance'], imbalance_bins) - 1 \n",
    "\n",
    "data[['Datetime', 'Demand', 'Supply', 'Imbalance', 'Price', 'State']].to_csv('simulated_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8bff6ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 53480\n",
      "Episode 2: Total Reward = 120540\n",
      "Episode 3: Total Reward = 161520\n",
      "Episode 4: Total Reward = 185740\n",
      "Episode 5: Total Reward = 207490\n",
      "Episode 6: Total Reward = 217260\n",
      "Episode 7: Total Reward = 226360\n",
      "Episode 8: Total Reward = 231800\n",
      "Episode 9: Total Reward = 240200\n",
      "Episode 10: Total Reward = 242110\n",
      "Episode 11: Total Reward = 242530\n",
      "Episode 12: Total Reward = 246270\n",
      "Episode 13: Total Reward = 243420\n",
      "Episode 14: Total Reward = 242170\n",
      "Episode 15: Total Reward = 243000\n",
      "Episode 16: Total Reward = 243370\n",
      "Episode 17: Total Reward = 246070\n",
      "Episode 18: Total Reward = 244630\n",
      "Episode 19: Total Reward = 243740\n",
      "Episode 20: Total Reward = 243220\n",
      "Episode 21: Total Reward = 243740\n",
      "Episode 22: Total Reward = 245420\n",
      "Episode 23: Total Reward = 246500\n",
      "Episode 24: Total Reward = 244490\n",
      "Episode 25: Total Reward = 243790\n",
      "Episode 26: Total Reward = 245690\n",
      "Episode 27: Total Reward = 243910\n",
      "Episode 28: Total Reward = 245990\n",
      "Episode 29: Total Reward = 244910\n",
      "Episode 30: Total Reward = 245130\n",
      "Episode 31: Total Reward = 246770\n",
      "Episode 32: Total Reward = 245150\n",
      "Episode 33: Total Reward = 244390\n",
      "Episode 34: Total Reward = 245910\n",
      "Episode 35: Total Reward = 245410\n",
      "Episode 36: Total Reward = 246050\n",
      "Episode 37: Total Reward = 245610\n",
      "Episode 38: Total Reward = 243590\n",
      "Episode 39: Total Reward = 243310\n",
      "Episode 40: Total Reward = 244190\n",
      "Episode 41: Total Reward = 244130\n",
      "Episode 42: Total Reward = 246090\n",
      "Episode 43: Total Reward = 245050\n",
      "Episode 44: Total Reward = 246210\n",
      "Episode 45: Total Reward = 243810\n",
      "Episode 46: Total Reward = 242470\n",
      "Episode 47: Total Reward = 244870\n",
      "Episode 48: Total Reward = 246190\n",
      "Episode 49: Total Reward = 244650\n",
      "Episode 50: Total Reward = 244270\n",
      "Episode 51: Total Reward = 244890\n",
      "Episode 52: Total Reward = 244870\n",
      "Episode 53: Total Reward = 244390\n",
      "Episode 54: Total Reward = 245490\n",
      "Episode 55: Total Reward = 243590\n",
      "Episode 56: Total Reward = 246030\n",
      "Episode 57: Total Reward = 244150\n",
      "Episode 58: Total Reward = 246570\n",
      "Episode 59: Total Reward = 244250\n",
      "Episode 60: Total Reward = 244950\n",
      "Episode 61: Total Reward = 246670\n",
      "Episode 62: Total Reward = 247250\n",
      "Episode 63: Total Reward = 246270\n",
      "Episode 64: Total Reward = 247610\n",
      "Episode 65: Total Reward = 247010\n",
      "Episode 66: Total Reward = 246490\n",
      "Episode 67: Total Reward = 245370\n",
      "Episode 68: Total Reward = 246250\n",
      "Episode 69: Total Reward = 244430\n",
      "Episode 70: Total Reward = 247750\n",
      "Episode 71: Total Reward = 248490\n",
      "Episode 72: Total Reward = 244010\n",
      "Episode 73: Total Reward = 245470\n",
      "Episode 74: Total Reward = 243910\n",
      "Episode 75: Total Reward = 245110\n",
      "Episode 76: Total Reward = 244250\n",
      "Episode 77: Total Reward = 244770\n",
      "Episode 78: Total Reward = 247450\n",
      "Episode 79: Total Reward = 245450\n",
      "Episode 80: Total Reward = 245650\n",
      "Episode 81: Total Reward = 246050\n",
      "Episode 82: Total Reward = 243370\n",
      "Episode 83: Total Reward = 245330\n",
      "Episode 84: Total Reward = 244650\n",
      "Episode 85: Total Reward = 246250\n",
      "Episode 86: Total Reward = 246770\n",
      "Episode 87: Total Reward = 242630\n",
      "Episode 88: Total Reward = 246650\n",
      "Episode 89: Total Reward = 244730\n",
      "Episode 90: Total Reward = 243670\n",
      "Episode 91: Total Reward = 245610\n",
      "Episode 92: Total Reward = 246450\n",
      "Episode 93: Total Reward = 242330\n",
      "Episode 94: Total Reward = 246010\n",
      "Episode 95: Total Reward = 247470\n",
      "Episode 96: Total Reward = 243610\n",
      "Episode 97: Total Reward = 243050\n",
      "Episode 98: Total Reward = 245630\n",
      "Episode 99: Total Reward = 247690\n",
      "Episode 100: Total Reward = 249630\n",
      "price: 0        85.386312\n",
      "1        96.800000\n",
      "2        85.802056\n",
      "3        80.000000\n",
      "4        96.800000\n",
      "           ...    \n",
      "1995     91.135278\n",
      "1996    134.778951\n",
      "1997     90.223926\n",
      "1998     91.729211\n",
      "1999    100.000000\n",
      "Name: Price, Length: 2000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = data.iloc[:2000].reset_index(drop=True)\n",
    "def adjust_price_and_demand(row, action):\n",
    "    min_price, max_price = 80, 150\n",
    "    \n",
    "    if action == 0:  # Increase Price\n",
    "        new_price = min(row['Price'] * 1.1, max_price) \n",
    "        new_demand = max(row['Demand'] * 0.9, 0)  \n",
    "    elif action == 1:  # Decrease Price\n",
    "        new_price = max(row['Price'] * 0.9, min_price)  \n",
    "        new_demand = row['Demand'] * 1.1 \n",
    "    else:  # Hold Price\n",
    "        new_price = row['Price']\n",
    "        new_demand = row['Demand']\n",
    "    \n",
    "    return new_price, new_demand\n",
    "\n",
    "\n",
    "\n",
    "def calculate_reward_with_price(imbalance, fossil_production, price, action):\n",
    "    reward = 0\n",
    "\n",
    "    if abs(imbalance) < 200:  \n",
    "        reward += 100\n",
    "    if 80 <= price <= 150:\n",
    "        reward += 100\n",
    "    else:\n",
    "        reward -= 50\n",
    "    \n",
    "    if action == 0:  # Increase Price\n",
    "        reward += 50 if imbalance > 50 else -10\n",
    "    elif action == 1:  # Decrease Price\n",
    "        reward += 50 if imbalance < -50 else -10\n",
    "    elif action == 2:  # Hold Price\n",
    "        reward += 50 if abs(imbalance) < 50 else -10\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "#Q-Learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_space, action_space, learning_rate=0.1, discount_factor=0.8, exploration_rate=1.0):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate  \n",
    "        self.q_table = np.zeros((state_space, action_space))  \n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return np.random.randint(self.action_space)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  \n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state, done):\n",
    "        best_next_action = np.argmax(self.q_table[next_state]) \n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action] * (1 - done)\n",
    "        td_error = td_target - self.q_table[state][action]  \n",
    "        self.q_table[state][action] += self.learning_rate * td_error \n",
    "\n",
    "state_space = len(imbalance_bins)  \n",
    "action_space = 3  \n",
    "agent = QLearningAgent(\n",
    "    state_space=state_space,\n",
    "    action_space=action_space,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.8, \n",
    "    exploration_rate=1.0  \n",
    ")\n",
    "\n",
    "num_episodes = 100\n",
    "episode_rewards = []  \n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    total_reward = 0\n",
    "\n",
    "    for idx in range(len(data) - 1):\n",
    "        current_row = data.iloc[idx]\n",
    "        next_row = data.iloc[idx + 1]\n",
    "        state = current_row['State']\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        new_price, new_demand = adjust_price_and_demand(current_row, action)\n",
    "        data.at[idx, 'Price'] = new_price  # update price each step\n",
    "        data.at[idx, 'Demand'] = new_demand  # update demand each step\n",
    "\n",
    "        adjusted_supply = current_row['Supply']\n",
    "        imbalance = adjusted_supply - new_demand\n",
    "\n",
    "        reward = calculate_reward_with_price(imbalance, current_row['FossilProduction'], new_price, action)\n",
    "\n",
    "        # get next state\n",
    "        next_state = next_row['State']\n",
    "        done = idx == len(data) - 2\n",
    "\n",
    "        # update Q table\n",
    "        agent.update_q_value(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    \n",
    "'''\n",
    "# take a look for reward trend after learning\n",
    "plt.plot(range(num_episodes), episode_rewards)\n",
    "plt.title(\"Total Rewards Over Episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()\n",
    "\n",
    "# take a look at ele-price after learning\n",
    "plt.plot(data['Datetime'], data['Price'])\n",
    "plt.title(\"Electricity Price Over Time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()\n",
    "'''\n",
    "print(\"price:\",data['Price'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd243c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be18d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
